{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "4ea712d0-9f9d-4d68-a572-6ac2b119006a",
    "_uuid": "ab9733987f40fa168b7c1e84964dca054f6c3fed"
   },
   "source": [
    "As always, first step I am going to explore the data, and see what does the training set looks like, also I'll share some ideas here.\n",
    "* Read in some helpful NLP libraries & our dataset\n",
    "* Find out how often each author uses each word\n",
    "* Find out what is the mean length of each sentences for each author\n",
    "* Use that to guess which author wrote a sentence\n",
    "\n",
    "\n",
    "\n",
    "Ready? Let's get started! :D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "52783943-8073-4d7d-815d-868500ecf850",
    "_uuid": "12b7cdc56b317df2ef4392c3b304a51ceda10caf"
   },
   "source": [
    "## Read in some helpful NLP libraries & our dataset\n",
    "\n",
    "For this tutorial, I'm going to be using the Natural Language Toolkit, also called the \"NLTK\". It's an open-source Python library for analyzing language data. The really nice thing about the NLTK is that it has a really helpful book that goes step-by-step through a lot of the common NLP tasks. Even better: you can get the book for free [here](http://www.nltk.org/book/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "e5d6cdb9-0a56-44b2-99da-ae2432f89b9f",
    "_uuid": "eb67d93a8e3ae84d499f706568d7611627f5308a",
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nltk'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-4d442b0b3c0e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# read in some helpful libraries\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mnltk\u001b[0m \u001b[1;31m# the natural langauage toolkit, open-source NLP\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m \u001b[1;31m# dataframes\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'nltk'"
     ]
    }
   ],
   "source": [
    "# read in some helpful libraries\n",
    "import nltk # the natural langauage toolkit, open-source NLP\n",
    "import pandas as pd # dataframes\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "### Read in the data\n",
    "\n",
    "# read our data into a dataframe\n",
    "texts = pd.read_csv(\"input/train.csv\")\n",
    "\n",
    "# look at the first few rows\n",
    "texts.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6b4dc91002ccddcc855ab9272d2e9e41353b3492",
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1d211dfd1e4121c41b5b816453570d92b098fadf"
   },
   "source": [
    "## Find out the mean length distribution of each author\n",
    "\n",
    "Let's look at if the number of words contained in one sample text is different.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_uuid": "9e844c0d89fbb0fccf96ab49f04657ffe0b679a0"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'texts' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-b4e9806d672c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mbyAuthor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtexts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"author\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mwordlength\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mwordmean\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroup\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbyAuthor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mlength\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'texts' is not defined"
     ]
    }
   ],
   "source": [
    "byAuthor = texts.groupby(\"author\")\n",
    "wordlength = dict()\n",
    "wordmean = dict()\n",
    "for name, group in byAuthor:\n",
    "    length = []\n",
    "    sentences = group['text']\n",
    "    for sentence in sentences:\n",
    "        token = nltk.tokenize.word_tokenize(sentence)\n",
    "        length.append(len(token))\n",
    "    wordlength[name] = length\n",
    "    wordmean[name] = sum(length)/len(length)\n",
    "print(wordmean)\n",
    "plt.hist(wordlength['EAP'],range(0,100,2))\n",
    "plt.hist(wordlength['MWS'],range(0,100,2))\n",
    "plt.hist(wordlength['HPL'],range(0,100,2))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "75f43a03-d83d-4551-ac9a-4ac4d0764b32",
    "_uuid": "668599ac03c8b70093377c7250cd34e93ba6cffe"
   },
   "source": [
    "## Find out how often each author uses each word\n",
    "\n",
    "A lot of NLP applications rely on counting how often certain words are used. (The fancy term for this is \"word frequency\".) Let's look at the word frequency for each of the authors in our dataset. The NLTK has lots of nice built-in functions and data structures for this that we can make use of."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "0456a8ef-0aca-4b40-8199-0e4fdec81671",
    "_uuid": "025540f501355cfb33e6bc256ad3c0980793d0fd"
   },
   "outputs": [],
   "source": [
    "### Split data\n",
    "\n",
    "# split the data by author\n",
    "byAuthor = texts.groupby(\"author\")\n",
    "\n",
    "### Tokenize (split into individual words) our text\n",
    "\n",
    "# word frequency by author\n",
    "wordFreqByAuthor = nltk.probability.ConditionalFreqDist()\n",
    "\n",
    "# for each author...\n",
    "for name, group in byAuthor:\n",
    "    # get all of the sentences they wrote and collapse them into a\n",
    "    # single long string\n",
    "    sentences = group['text'].str.cat(sep = ' ')\n",
    "    \n",
    "    # convert everything to lower case (so \"The\" and \"the\" get counted as \n",
    "    # the same word rather than two different words)\n",
    "    sentences = sentences.lower()\n",
    "    \n",
    "    # split the text into individual tokens    \n",
    "    tokens = nltk.tokenize.word_tokenize(sentences)\n",
    "    \n",
    "    # calculate the frequency of each token\n",
    "    frequency = nltk.FreqDist(tokens)\n",
    "\n",
    "    # add the frequencies for each author to our dictionary\n",
    "    wordFreqByAuthor[name] = (frequency)\n",
    "    \n",
    "# now we have an dictionary where each entry is the frequency distrobution\n",
    "# of words for a specific author.     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "a0d3c955-c7ac-4460-befa-7743d7ba37ec",
    "_uuid": "1c10083be2d0bb1d8123e71cda822e35f7809bae"
   },
   "source": [
    "Now we can look at how often each writer uses specific words. Since this is a Halloween competition, how about \"blood\", \"scream\" and \"fear\"? üëªüò®üßõ‚Äç‚ôÄÔ∏è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_cell_guid": "56d4ab79-3361-4410-8d89-47b014b08357",
    "_uuid": "356327c380228999a0476a82076486535891d021"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blood: EAP\n",
      "0.00014646397201676582\n",
      "blood: HPL\n",
      "0.00022992337803427008\n",
      "blood: MWS\n",
      "0.00022773011333545174\n",
      "\n",
      "scream: EAP\n",
      "1.7231055531384214e-05\n",
      "scream: HPL\n",
      "9.196935121370803e-05\n",
      "scream: MWS\n",
      "2.6480245736680435e-05\n",
      "\n",
      "fear: EAP\n",
      "0.00010338633318830528\n",
      "fear: HPL\n",
      "0.0005748084450856752\n",
      "fear: MWS\n",
      "0.0006196377502383222\n"
     ]
    }
   ],
   "source": [
    "# see how often each author says \"blood\"\n",
    "for i in wordFreqByAuthor.keys():\n",
    "    print(\"blood: \" + i)\n",
    "    print(wordFreqByAuthor[i].freq('blood'))\n",
    "\n",
    "# print a blank line\n",
    "print()\n",
    "\n",
    "# see how often each author says \"scream\"\n",
    "for i in wordFreqByAuthor.keys():\n",
    "    print(\"scream: \" + i)\n",
    "    print(wordFreqByAuthor[i].freq('scream'))\n",
    "    \n",
    "# print a blank line\n",
    "print()\n",
    "\n",
    "# see how often each author says \"fear\"\n",
    "for i in wordFreqByAuthor.keys():\n",
    "    print(\"fear: \" + i)\n",
    "    print(wordFreqByAuthor[i].freq('fear'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "bcf7a4fe-48c7-4351-bf43-f9e1d9272592",
    "_uuid": "eb4e859307aa90d239ed9066180a0df958916e19"
   },
   "source": [
    "## Use word frequency to guess which author wrote a sentence\n",
    "\n",
    "The general idea is is that different people tend to use different words more or less often. (I had a beloved college professor that was especially fond of \"gestalt\".) If you're not sure who said something but it has a lot of words one person uses a lot in it, then you might guess that they were the one who wrote it. \n",
    "\n",
    "Let's use this general principle to guess who might have been more likely to write the sentence \"It was a dark and stormy night.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_cell_guid": "221c3934-12f9-4e8f-8e91-035c7fd97b28",
    "_uuid": "9f17c73bd55582b518ddbcfdba4053b1dc0182f3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'HPL'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# One way to guess authorship is to use the joint probabilty that each \n",
    "# author used each word in a given sentence.\n",
    "\n",
    "# first, let's start with a test sentence\n",
    "testSentence = \"It was a dark and stormy night.\"\n",
    "\n",
    "# and then lowercase & tokenize our test sentence\n",
    "preProcessedTestSentence = nltk.tokenize.word_tokenize(testSentence.lower())\n",
    "\n",
    "# create an empy dataframe to put our output in\n",
    "testProbailities = pd.DataFrame(columns = ['author','word','probability'])\n",
    "\n",
    "# For each author...\n",
    "for i in wordFreqByAuthor.keys():\n",
    "    # for each word in our test sentence...\n",
    "    for j  in preProcessedTestSentence:\n",
    "        # find out how frequently the author used that word\n",
    "        wordFreq = wordFreqByAuthor[i].freq(j)\n",
    "        # and add a very small amount to every prob. so none of them are 0\n",
    "        smoothedWordFreq = wordFreq + 0.000001\n",
    "        # add the author, word and smoothed freq. to our dataframe\n",
    "        output = pd.DataFrame([[i, j, smoothedWordFreq]], columns = ['author','word','probability'])\n",
    "        testProbailities = testProbailities.append(output, ignore_index = True)\n",
    "\n",
    "# empty dataframe for the probability that each author wrote the sentence\n",
    "testProbailitiesByAuthor = pd.DataFrame(columns = ['author','jointProbability'])\n",
    "\n",
    "# now let's group the dataframe with our frequency by author\n",
    "for i in wordFreqByAuthor.keys():\n",
    "    # get the joint probability that each author wrote each word\n",
    "    oneAuthor = testProbailities.query('author == \"' + i + '\"')\n",
    "    jointProbability = oneAuthor.product(numeric_only = True)[0]\n",
    "    \n",
    "    # and add that to our dataframe\n",
    "    output = pd.DataFrame([[i, jointProbability]], columns = ['author','jointProbability'])\n",
    "    testProbailitiesByAuthor = testProbailitiesByAuthor.append(output, ignore_index = True)\n",
    "\n",
    "# and our winner is...\n",
    "testProbailitiesByAuthor.loc[testProbailitiesByAuthor['jointProbability'].idxmax(),'author']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "fca66518-050f-4484-b572-272ed911b805",
    "_uuid": "e6eb3ec205947d5a15a8288c6eefc4f811eafeb4"
   },
   "source": [
    "So based on what we've seen in our training data, it looks like of our three authors, H.P. Lovecraft was the most likely to write the sentence \"It was a dark and stormy night\"."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
